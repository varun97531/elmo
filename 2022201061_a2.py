# -*- coding: utf-8 -*-
"""2022201061_a2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JnTeA2Hrwnx-7lt1lsNnP6No-Vs8QjKN
"""

import numpy as np
import pandas as pd
import gensim
import os
import csv
import nltk

nltk.download('punkt')

from google.colab import drive
drive.mount('/content/gdrive')

path = r"/content/gdrive/My Drive/ANLP-2/train.csv"

with open(path, 'r') as file:
    train_dataset = csv.reader(file)
    next(train_dataset)
    columns = ['value', 'Sentences']
    df = pd.DataFrame(train_dataset, columns=columns)

# df = df[1:]

df['Sentences'][0]

import spacy, re
from bs4 import BeautifulSoup

nlp = spacy.load("en_core_web_sm")

def preprocess_text(text):
    # text = BeautifulSoup(text, "html.parser")
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\\', ' ', text)  # Remove '\\'
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters except whitespace

    return text


def clean_text(text):
    text = preprocess_text(text)
    doc = nlp(text)
    # tokens = [token.lemma_.strip() for token in doc if not token.is_punct and not token.is_stop and token.lemma_.strip() != '']
    tokens = [token.lemma_.strip() for token in doc if not token.is_punct and token.lemma_.strip() != '']
    return tokens

df["Sentences"] = df["Sentences"].str.lower()

html_content = "<p>This is <strong>HTML</strong> content.</p>"

print(clean_text(html_content))

df.shape

# import string,time
# import re
# def remove_tags(text):
#     return text.strip().split()

# punctuations = string.punctuation
# punctuations += "“”•™-_0123456789—"
# def remove_punctuation(text):

#     for char in punctuations:
#         text = text.replace(char,'')
#     return text

# df["Sentences"] = df["Sentences"].str.lower()
# df['Sentences'] = df['Sentences'].apply(remove_punctuation)
# df['Sentences'] = df['Sentences'].apply(remove_tags)

# df["Sentences"] = df["Sentences"].apply(clean_text)

length = len(df)
l1, l2 = int(length*0.7), int(length*0.2)
train, test, dev = df[:l1], df[l1:l1+l2], df[l1+l2:]
print(len(train), len(test), len(dev))

dev = dev.reset_index(drop=True)
test = test.reset_index(drop=True)
train = train.reset_index(drop=True)

dev['Sentences'][0]

# train['Sentences'] = train['Sentences'].apply(clean_text)

# train_sentence = train['Sentences']
# train_value = train['value']

# train_sentence[0]

# file_path = r"/content/gdrive/My Drive/train_sentence.txt"

# with open(file_path, "w") as file:
#     for sublist in train_sentence:
#         file.write(" ".join(sublist) + "\n")

# file_path = r"/content/gdrive/My Drive/train_value.txt"

# with open(file_path, "w") as file:
#     for item in train_value:
#         file.write(str(item) + "\n")

file_path = r"/content/gdrive/My Drive/train_sentence.txt"

lines = []
with open(file_path, "r") as file:
    lines = file.readlines()
train_sentence = []
for line in lines:
    train_sentence.append(line.split())
print(train_sentence[:2])


file_path = r"/content/gdrive/My Drive/train_value.txt"
lines = []
with open(file_path, "r") as file:
    lines = file.readlines()
train_value = []
for line in lines:
    train_value.append(int(line))

import gensim.downloader as api
import torch
import torch.nn as nn

from gensim.models import KeyedVectors
import gensim.downloader as download
glove_model = download.load("glove-wiki-gigaword-100")

glove_embedding_dim = 100

word2idx = {'<PAD>': 0, '<UNK>': 1, '<EOS>' : 2, '<SOS>' : 3}
idx2word = {0 : '<PAD>', 1:'<UNK>', 2 : '<EOS>', 3 : '<SOS>'}
idx = 4
vocabulary = set()
vocabulary.add('<PAD>')
vocabulary.add('<UNK>')
vocabulary.add('<SOS>')
vocabulary.add('<EOS>')
for sent in train_sentence:
    for word in sent:
        vocabulary.add(word)

idx = 0
for word in vocabulary:
    word2idx[word] = idx
    idx2word[idx] = word
    idx += 1

word2idx['<PAD>']

print(len(vocabulary), vocabulary)

unk_vector = np.ones(100)
pad_vector = np.zeros(100)
sos_vector = np.random.rand(100)
eos_vector = np.random.rand(100)

embedding_vector = {'<PAD>':unk_vector, '<SOS>':sos_vector, '<UNK>':unk_vector, '<EOS>':eos_vector}

def get_word_embedding(word):
      if word in glove_model:
          return glove_model[word]
      elif word in embedding_vector:
          return embedding_vector[word]
      else:
          return unk_vector

embedding_matrix = []
for word in vocabulary:
    embedding_matrix.append(get_word_embedding(word))

embedding_matrix = torch.tensor(embedding_matrix)

# embedding_matrix = np.array(embedding_matrix)

import torch

def custom_collate_fn(sentences):
    # print(sentences)
    max_length = max(len(sentence) for sentence in sentences)
    padded_inputs = []
    padded_outputs = []
    for sentence in sentences:

        inputs, outputs = sentence[:-1], sentence
        # print(inputs)
        # print(outputs)
        # print('='*50)
        # padded_sentence = [word2idx['<SOS>']] + [word2idx[word] if word in word2idx else word2idx['<UNK>'] for word in sentence] + [word2idx['<PAD>'] * (max_length - len(sentence))] + [word2idx['<EOS>']]
        sos_token = [word2idx['<SOS>']]
        sentence_tokens = [word2idx[word] if word in word2idx else word2idx['<UNK>'] for word in inputs]
        pad_tokens = [word2idx['<PAD>']] * (max_length - len(inputs) -1)
        # eos_token = [word2idx['<EOS>']]
        combined_list = sos_token + sentence_tokens + pad_tokens
        padded_inputs.append(combined_list)

        # sos_token = [word2idx['<SOS>']]
        sentence_tokens = [word2idx[word] if word in word2idx else word2idx['<UNK>'] for word in outputs]
        pad_tokens = [word2idx['<PAD>']] * (max_length - len(outputs))
        eos_token = [word2idx['<EOS>']]
        # combined_list = sentence_tokens + pad_tokens + eos_token
        combined_list = sentence_tokens + pad_tokens
        padded_outputs.append(combined_list)

    # print(torch.tensor(padded_inputs).shape, torch.tensor(padded_outputs).shape)
    return torch.tensor(padded_inputs), torch.tensor(padded_outputs)

inp, out = custom_collate_fn(train_sentence[:2])
# inp, out

from torch.utils.data import DataLoader
batch_size = 32
dataloader = DataLoader(train_sentence, batch_size=batch_size, collate_fn=custom_collate_fn)

len(train_sentence), len(dataloader), len(train_sentence)/batch_size

idx2word[2], idx2word[5], idx2word[7]

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# import torch

# index_sentences = torch.tensor([[1, 2, 3, 4], [4, 5, 6, 7]])
# x = index_sentences.view(-1, len(x[0]))
# embeddings = embedding_matrix[x]
# embeddings.shape, embeddings

# import torch
# import torch.nn as nn

# class ELMo(nn.Module):
#     def __init__(self, vocab_size, embedding_dim, hidden_dim=100, num_layers=1, dropout=0.3):
#         super(ELMo, self).__init__()

#         # Word embeddings layer (you can replace this with Word2Vec embeddings)
#         self.hidden_dim = hidden_dim
#         self.vocab_size = vocab_size
#         self.embedding = nn.Embedding(vocab_size, embedding_dim)
#         self.lstm_forward_1 = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True).double()
#         self.lstm_forward_2 = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True).double()
#         self.lstm_backward_1 = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True).double()
#         self.lstm_backward_2 = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True).double()
#         self.scalars = nn.ParameterList([nn.Parameter(torch.ones(2)) for _ in range(num_layers+1)]).double()
#         # self.linear_layer = nn.Linear(hidden_dim*2, vocab_size).double()  # Set the output dimension to 100
#         self.dropout = nn.Dropout(dropout).double()

#     def forward(self, input):
#         input = input.view(-1, len(input[0]))
#         embeddings = embedding_matrix[input]
#         lstm_outputs = [[embeddings, embeddings]]

#         lstm_out_forward_1, _ = self.lstm_forward_1(embeddings)
#         # embeddings_reversed_1 = torch.flip(embeddings, [1])
#         lstm_out_backward_1, _ = self.lstm_backward_1(torch.flip(embeddings, [1]))
#         lstm_out_backward_1 = torch.flip(lstm_out_backward_1, [1])

#         lstm_out_forward_2, _ = self.lstm_forward_2(lstm_out_forward_1)
#         # embeddings_reversed_2 = torch.flip(lstm_out_backward_1, [1])
#         lstm_out_backward_2, _ = self.lstm_backward_2(torch.flip(lstm_out_backward_1, [1]))
#         lstm_out_backward_2 = torch.flip(lstm_out_backward_2, [1])

        # lstm_outputs.append([lstm_out_forward_1, lstm_out_backward_1])
        # lstm_outputs.append([lstm_out_forward_2, lstm_out_backward_2])

#         combined_output = torch.zeros((batch_size, len(input[0]), 200), dtype=torch.float64).to(device)

        # for i, lstm_out in enumerate(lstm_outputs):
        #     tensor1 = self.scalars[i][0] * lstm_out[0]
        #     tensor2 = self.scalars[i][1] * lstm_out[1]

        #     concatenated = torch.cat((tensor1, tensor2), dim=2)
        #     print(concatenated.shape)
        #     combined_output += concatenated

#         print(combined_output.shape)
#         linear_layer = nn.Linear(self.hidden_dim*2, self.vocab_size*len(input[0])).double().to(device)
#         linear_output = linear_layer(combined_output).view(-1, batch_size).view(-1, len(input[0]))

#         return linear_output

"""PARAMETERS TRAINABLE"""

class ELMo(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, weights_matrix):
        super(ELMo, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.embedding_layer = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)
        self.scalars = nn.ParameterList([nn.Parameter(torch.ones(1)) for _ in range(num_layers+1)]).double()
        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.lstm2 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.output_layer = nn.Linear(embedding_dim, vocab_size).double()
        # self.alphas = [0.2, 0.4, 0.4]

    def forward(self, x):
        embedded_x = self.embedding_layer(x)
        lstm_out1, _ = self.lstm1(embedded_x)
        lstm_out2, _ = self.lstm2(lstm_out1)
        # print("lstm2 : ", lstm_out2.shape)
        final_out = self.output_layer(lstm_out2)
        embed_out = embedded_x*self.scalars[0] + lstm_out1*self.scalars[1] + lstm_out2*self.scalars[2]
        return final_out, embed_out

import torch
import torch.optim as optim
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence
embedding_dim = 100
hidden_dim = 50
num_layers = 2
batch_size_lstm = 32
vocab_size = len(vocabulary)
embedding_matrix = embedding_matrix.to(device)
model = ELMo(vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix).to(device)

# Define loss function and optimizer
loss_function = nn.CrossEntropyLoss()
learning_rate = 0.01
optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)

for batch_input_embeddings, batch_target_embeddings in (dataloader):
    print(batch_input_embeddings, batch_target_embeddings)
    print(batch_input_embeddings.shape, batch_target_embeddings.shape)
    break

num_epochs = 5
path = "/content/gdrive/My Drive/ass2_model_parameters.pth"
prev_loss = 1000
from tqdm import tqdm

for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count=0
    mini = 100
    for batch_input_embeddings, batch_target_embeddings in (dataloader):
        loss = 0
        batch_input_embeddings = batch_input_embeddings.to(device)
        batch_target_embeddings = batch_target_embeddings.to(device)

        # print(batch_input_embeddings.shape, batch_target_embeddings.shape)
        optimizer.zero_grad()
        outputs, _ = model(torch.tensor(batch_input_embeddings))
        # outputs = outputs.float()
        # print(batch_target_embeddings.shape, outputs.shape)
        loss = loss_function(outputs.view(-1,vocab_size),batch_target_embeddings.contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    if(prev_loss > total_loss):
        torch.save(model.state_dict(), path)
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader)}')

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class downstream_task(nn.Module):
    def __init__(self, embedding_dim, vocab_size, no_of_values, num_layers=2):
        super(downstream_task, self).__init__()
        self.vocab_size = vocab_size
        self.no_of_values = no_of_values
        # self.lstm = nn.LSTM(embedding_dim, embedding_dim, num_layers, batch_first=True).double()
        self.linear = nn.Linear(embedding_dim, no_of_values).double()

    def forward(self, input):
        # lstm_out, (hidden, cell_state) = self.lstm(input)
        # print("hidden shape", hidden.shape)
        # linear_out = self.linear(hidden[1])
        # print("linear_out shape", linear_out.shape)

        linear_out = self.linear(input)
        return linear_out

model_downstream = downstream_task(embedding_dim, vocab_size, 4).to(device)
no_of_values = 4
loss_function = nn.CrossEntropyLoss()
learning_rate = 0.001
optimizer = optim.Adam(model_downstream.parameters(), lr=learning_rate, weight_decay=1e-5)
train_val = [int(val)-1 for val in train_value]

import torch

def custom_collate_fn(batch):
    max_length = max(len(sentence) for sentence, _ in batch)
    padded_inputs = []
    outputs = []
    for sentence, label in batch:
        # print(sentence)
        sos_token = [word2idx['<SOS>']]
        sentence_tokens = [word2idx[word] if word in word2idx else word2idx['<UNK>'] for word in sentence]
        pad_tokens = [word2idx['<PAD>']] * (max_length - len(sentence))
        combined_list = sos_token + sentence_tokens + pad_tokens

        padded_inputs.append(combined_list)
        outputs.append(label)


    # print(torch.tensor(padded_inputs).shape, torch.tensor(outputs).shape)
    return torch.tensor(padded_inputs), torch.tensor(outputs)

for batch_input_embeddings, batch_target in (dataloader_downstream):
    print(batch_input_embeddings, batch_target)
    print(batch_input_embeddings.shape, batch_target.shape)
    break

num_epochs = 5
prev_loss = 1000
from tqdm import tqdm
for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count = 0
    mini = 100
    for batch_input_embeddings, batch_target in (dataloader_downstream):
        loss = 0
        batch_input_embeddings = torch.tensor(batch_input_embeddings).to(device)
        batch_target = batch_target.to(device)
        optimizer.zero_grad()
        _, outputs = model(batch_input_embeddings)
        # print("outputs",outputs.shape)

        outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
        # print("outputs_downstream",outputs_downstream.shape)
        outputs_downstream = model_downstream(outputs_sum)
        # print("outputs_downstream",outputs_downstream.shape)
        loss = loss_function(outputs_downstream, batch_target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if prev_loss > total_loss:
        torch.save(model_downstream.state_dict(), "/content/gdrive/My Drive/ass2_downstream_parameters_model.pth")
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader_downstream)}')

test['Sentences'] = test['Sentences'].apply(clean_text)

test_sentence = test['Sentences']
test_value = test['value']

custom_dataset_test = CustomDataset(test_sentence, test_value)
dataloader_downstream_test = DataLoader(custom_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)

count = 0
total = 0

true = []
predicted = []
# for batch_input_embeddings, batch_target in tqdm(dataloader_downstream_test):

for batch_input_embeddings, batch_target in dataloader_downstream_test:
    batch_input_embeddings = batch_input_embeddings.to(device)
    batch_target = batch_target.to(device)
    _, outputs = model(batch_input_embeddings)
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]

    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()

    # print("Input 1 : ", batch_input_embeddings.shape)
    # print("Output 1 : ", outputs.shape)
    # print(batch_input_embeddings)
    outputs_down = np.argmax((torch.tensor(outputs_downstream)).cpu().numpy())
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
    # outputs_sum = outputs_sum.cpu().numpy()
    # print("argmax : ", outputs_down)

    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()
    # print("Output 2 : ", outputs_downstream.shape)



    true.append(batch_target.cpu())  # Move the true labels to CPU
    predicted.append(outputs_down)
    if outputs_down == batch_target:
        count += 1
    total += 1
    # if(total == 10):
    #     break
    if(total%100 == 0):
        print(count / total)

print("accuracy : ", count / total)

print(true)
print(predicted)

true1 = true
true = [int(val) for val in true]

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# Calculate precision, recall, and F1 score with 'micro' average
precision = precision_score(true, predicted, average='micro')
recall = recall_score(true, predicted, average='micro')
f1 = f1_score(true, predicted, average='micro')
conf_matrix = confusion_matrix(true, predicted)

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:")
print(conf_matrix)

"""ELMO WITH DROPOUT AND PARAMETER TRAINED"""

class ELMo_dropout(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, weights_matrix):
        super(ELMo_dropout, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.embedding_layer = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)
        self.scalars = nn.ParameterList([nn.Parameter(torch.ones(1)) for _ in range(num_layers+1)]).double()
        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, dropout=0.3, bidirectional=True, batch_first=True).double()
        self.lstm2 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, dropout=0.3, bidirectional=True, batch_first=True).double()
        self.output_layer = nn.Linear(embedding_dim, vocab_size).double()
        # self.alphas = [0.2, 0.4, 0.4]

    def forward(self, x):
        embedded_x = self.embedding_layer(x)
        lstm_out1, _ = self.lstm1(embedded_x)
        lstm_out2, _ = self.lstm2(lstm_out1)
        # print("lstm2 : ", lstm_out2.shape)
        final_out = self.output_layer(lstm_out2)
        embed_out = embedded_x*self.scalars[0] + lstm_out1*self.scalars[1] + lstm_out2*self.scalars[2]
        return final_out, embed_out

model_dropout = ELMo_dropout(vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix).to(device)

# Define loss function and optimizer
loss_function = nn.CrossEntropyLoss()
# learning_rate = 0.001
optimizer = optim.Adam(model_dropout.parameters(), lr=0.01, weight_decay=1e-5)

num_epochs = 5
path = "/content/gdrive/My Drive/ass2_model_dropout.pth"
prev_loss = 1000
from tqdm import tqdm

for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count=0
    mini = 100
    for batch_input_embeddings, batch_target_embeddings in (dataloader):
        loss = 0
        batch_input_embeddings = batch_input_embeddings.to(device)
        batch_target_embeddings = batch_target_embeddings.to(device)

        # print(batch_input_embeddings.shape, batch_target_embeddings.shape)
        optimizer.zero_grad()
        outputs, _ = model_dropout(torch.tensor(batch_input_embeddings))
        # print(batch_target_embeddings.shape, outputs.shape)
        loss = loss_function(outputs.view(-1,vocab_size),batch_target_embeddings.contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    if(prev_loss > total_loss):
        torch.save(model.state_dict(), path)
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader)}')

# train_val = [int(val)-1 for val in train_value]
model_downstream_dropout = downstream_task(embedding_dim, vocab_size, 4).to(device)
optimizer = optim.Adam(model_downstream_dropout.parameters(), lr=0.01, weight_decay=1e-5)

num_epochs = 5
prev_loss = 1000
from tqdm import tqdm
for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count = 0
    mini = 100
    for batch_input_embeddings, batch_target in (dataloader_downstream_test):
        loss = 0
        batch_input_embeddings = torch.tensor(batch_input_embeddings).to(device)
        batch_target = batch_target.to(device)
        optimizer.zero_grad()
        _, outputs = model_dropout(batch_input_embeddings)
        # print("outputs",outputs.shape)

        outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
        # print("outputs_downstream",outputs_downstream.shape)
        outputs_downstream = model_downstream_dropout(outputs_sum)
        # print("outputs_downstream",outputs_downstream.shape)
        loss = loss_function(outputs_downstream, batch_target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if prev_loss > total_loss:
        torch.save(model_downstream_dropout.state_dict(), "/content/gdrive/My Drive/ass2_downstream_dropout_model.pth")
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader_downstream_test)}')

count = 0
total = 0

true = []
predicted = []
# for batch_input_embeddings, batch_target in tqdm(dataloader_downstream_test):

for batch_input_embeddings, batch_target in dataloader_downstream_test:
    batch_input_embeddings = batch_input_embeddings.to(device)
    batch_target = batch_target.to(device)
    _, outputs = model_dropout(batch_input_embeddings)
    outputs_downstream = model_downstream_dropout(outputs).cpu().detach().numpy()
    outputs_down = np.argmax(F.softmax(torch.tensor(outputs_downstream), dim=1).cpu().numpy())
    true.append(batch_target.cpu())  # Move the true labels to CPU
    predicted.append(outputs_down)
    if outputs_down == batch_target:
        count += 1
    total += 1
    if(total > 1000):
        break

print("accuracy : ", count / total)

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# Calculate precision, recall, and F1 score with 'micro' average
precision = precision_score(true, predicted, average='micro')
recall = recall_score(true, predicted, average='micro')
f1 = f1_score(true, predicted, average='micro')
conf_matrix = confusion_matrix(true, predicted)

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:")
print(conf_matrix)



"""MODEL WITH FIXED PARAMETRES"""

class ELMo(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, weights_matrix):
        super(ELMo, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.embedding_layer = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)
        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.lstm2 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.output_layer = nn.Linear(embedding_dim, vocab_size).double()
        self.alphas = [0.2, 0.4, 0.4]

    def forward(self, x):
        embedded_x = self.embedding_layer(x)
        lstm_out1, _ = self.lstm1(embedded_x)
        # print("lstm1 : ", lstm_out1.shape)
        lstm_out2, _ = self.lstm2(lstm_out1)
        # print("lstm2 : ", lstm_out2.shape)
        final_out = self.output_layer(lstm_out2)

        embed_out = embedded_x*self.alphas[0] + lstm_out1*self.alphas[1] + lstm_out2*self.alphas[2]
        return final_out, embed_out

import torch
import torch.optim as optim
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence
embedding_dim = 100
hidden_dim = 50
num_layers = 2
batch_size_lstm = 32
vocab_size = len(vocabulary)
embedding_matrix = embedding_matrix.to(device)
model = ELMo(vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix).to(device)

# Define loss function and optimizer
loss_function = nn.CrossEntropyLoss()
learning_rate = 0.001
optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)

idx2word[52237]

for batch_input_embeddings, batch_target_embeddings in (dataloader):
    print(batch_input_embeddings, batch_target_embeddings)
    print(batch_input_embeddings.shape, batch_target_embeddings.shape)
    break

num_epochs = 5
path = "/content/gdrive/My Drive/ass2_model.pth"
prev_loss = 1000
from tqdm import tqdm

for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count=0
    mini = 100
    for batch_input_embeddings, batch_target_embeddings in (dataloader):
        loss = 0
        batch_input_embeddings = batch_input_embeddings.to(device)
        batch_target_embeddings = batch_target_embeddings.to(device)

        # print(batch_input_embeddings.shape, batch_target_embeddings.shape)
        optimizer.zero_grad()
        outputs, _ = model(torch.tensor(batch_input_embeddings))
        outputs = outputs.long()
        # print(batch_target_embeddings.shape, outputs.shape)
        loss = loss_function(outputs.view(-1,vocab_size),batch_target_embeddings.contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    if(prev_loss > total_loss):
        torch.save(model.state_dict(), path)
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader)}')

num_epochs = 5
path = "/content/gdrive/My Drive/ass2_model.pth"
prev_loss = 1000
from tqdm import tqdm

for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count=0
    mini = 100
    for batch_input_embeddings, batch_target_embeddings in (dataloader):
        loss = 0
        batch_input_embeddings = batch_input_embeddings.to(device)
        batch_target_embeddings = batch_target_embeddings.to(device)

        # print(batch_input_embeddings.shape, batch_target_embeddings.shape)
        optimizer.zero_grad()
        outputs, _ = model(torch.tensor(batch_input_embeddings))
        # print(batch_target_embeddings.shape, outputs.shape)
        loss = loss_function(outputs.view(-1,vocab_size),batch_target_embeddings.contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    if(prev_loss > total_loss):
        torch.save(model.state_dict(), path)
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader)}')

# path = "/content/gdrive/My Drive/ass2_model.pth"
# model = torch.load(path, map_location=torch.device('cpu'))


model.load_state_dict(torch.load(path))

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class downstream_task(nn.Module):
    def __init__(self, embedding_dim, vocab_size, no_of_values, num_layers=2):
        super(downstream_task, self).__init__()
        self.vocab_size = vocab_size
        self.no_of_values = no_of_values
        # self.lstm = nn.LSTM(embedding_dim, embedding_dim, num_layers, batch_first=True).double()
        self.linear = nn.Linear(embedding_dim, no_of_values).double()

    def forward(self, input):
        # lstm_out, (hidden, cell_state) = self.lstm(input)
        # print("hidden shape", hidden.shape)
        # linear_out = self.linear(hidden[1])
        # print("linear_out shape", linear_out.shape)

        linear_out = self.linear(input)
        return linear_out

model_downstream = downstream_task(embedding_dim, vocab_size, 4).to(device)

# num_params = sum(p.numel() for p in model_downstream.parameters() if p.requires_grad)
# print(f"Number of trainable parameters: {num_params}")

no_of_values = 4

loss_function = nn.CrossEntropyLoss()
learning_rate = 0.001
optimizer = optim.Adam(model_downstream.parameters(), lr=learning_rate, weight_decay=1e-5)
train_val = [int(val)-1 for val in train_value]

import torch

def custom_collate_fn(batch):
    max_length = max(len(sentence) for sentence, _ in batch)
    padded_inputs = []
    outputs = []
    for sentence, label in batch:
        # print(sentence)
        sos_token = [word2idx['<SOS>']]
        sentence_tokens = [word2idx[word] if word in word2idx else word2idx['<UNK>'] for word in sentence]
        pad_tokens = [word2idx['<PAD>']] * (max_length - len(sentence))
        combined_list = sos_token + sentence_tokens + pad_tokens

        padded_inputs.append(combined_list)
        outputs.append(label)


    # print(torch.tensor(padded_inputs).shape, torch.tensor(outputs).shape)
    return torch.tensor(padded_inputs), torch.tensor(outputs)

from torch.utils.data import DataLoader, Dataset
batch_size = 32

class CustomDataset(Dataset):
    def __init__(self, sentences, labels):
        self.sentences = sentences
        self.labels = labels

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        return [self.sentences[idx], self.labels[idx]]

batch_size = 32
custom_dataset = CustomDataset(train_sentence, train_val)
dataloader_downstream = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)

for batch_input_embeddings, batch_target in (dataloader_downstream):
    print(batch_input_embeddings, batch_target)
    print(batch_input_embeddings.shape, batch_target.shape)
    break

num_epochs = 5
prev_loss = 1000
from tqdm import tqdm
for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count = 0
    mini = 100
    for batch_input_embeddings, batch_target in (dataloader_downstream):
        loss = 0
        batch_input_embeddings = torch.tensor(batch_input_embeddings).to(device)
        batch_target = batch_target.to(device)
        optimizer.zero_grad()
        _, outputs = model(batch_input_embeddings)
        # print("outputs",outputs.shape)

        outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
        # print("outputs_downstream",outputs_downstream.shape)
        outputs_downstream = model_downstream(outputs_sum)
        # print("outputs_downstream",outputs_downstream.shape)
        loss = loss_function(outputs_downstream, batch_target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if prev_loss > total_loss:
        torch.save(model_downstream.state_dict(), "/content/gdrive/My Drive/ass2_downstream_non_parametered_model.pth")
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader_downstream)}')

num_epochs = 5
prev_loss = 1000
from tqdm import tqdm
for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count = 0
    mini = 100
    for batch_input_embeddings, batch_target in (dataloader_downstream):
        loss = 0
        batch_input_embeddings = torch.tensor(batch_input_embeddings).to(device)
        batch_target = batch_target.to(device)
        optimizer.zero_grad()
        _, outputs = model(batch_input_embeddings)
        # print("outputs",outputs.shape)

        outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
        # print("outputs_downstream",outputs_downstream.shape)
        outputs_downstream = model_downstream(outputs_sum)
        # print("outputs_downstream",outputs_downstream.shape)
        loss = loss_function(outputs_downstream, batch_target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if prev_loss > total_loss:
        torch.save(model_downstream.state_dict(), "/content/gdrive/My Drive/ass2_downstream_non_parametered_model.pth")
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader_downstream)}')

num_epochs = 5
prev_loss = 1000
from tqdm import tqdm
for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count = 0
    mini = 100
    for batch_input_embeddings, batch_target in (dataloader_downstream):
        loss = 0
        batch_input_embeddings = torch.tensor(batch_input_embeddings).to(device)
        batch_target = batch_target.to(device)
        optimizer.zero_grad()
        _, outputs = model(batch_input_embeddings)
        print("outputs",outputs.shape)

        outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
        print("outputs_downstream 1 : ",outputs_downstream.shape)

        outputs_downstream = model_downstream(outputs_sum)
        print("outputs_downstream 2 : ",outputs_downstream.shape)
        print("Want : ", batch_target.shape)
        loss = loss_function(outputs_downstream, batch_target)
        break
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader_downstream)}')

# dev['Sentences'] = dev['Sentences'].apply(clean_text)

# dev_sentence = dev['Sentences']
# dev_value = dev['value']

count = 0
total = 0

true = []
predicted = []
# for batch_input_embeddings, batch_target in tqdm(dataloader_downstream_test):

for batch_input_embeddings, batch_target in dataloader_downstream_test:
    batch_input_embeddings = batch_input_embeddings.to(device)
    batch_target = batch_target.to(device)
    _, outputs = model(batch_input_embeddings)
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]

    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()

    # print("Input 1 : ", batch_input_embeddings.shape)
    # print("Output 1 : ", outputs.shape)
    # print(batch_input_embeddings)
    outputs_down = np.argmax((torch.tensor(outputs_downstream)).cpu().numpy())
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
    # outputs_sum = outputs_sum.cpu().numpy()
    # print("argmax : ", outputs_down)

    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()
    # print("Output 2 : ", outputs_downstream.shape)



    true.append(batch_target.cpu())  # Move the true labels to CPU
    predicted.append(outputs_down)
    if outputs_down == batch_target:
        count += 1
    total += 1
    # if(total == 10):
    #     break
    if(total%100 == 0):
        print(count / total)

print("accuracy : ", count / total)

print(true)
print(predicted)

true1 = true

# true

true = [int(val) for val in true]
true[0]

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# Calculate precision, recall, and F1 score with 'micro' average
precision = precision_score(true, predicted, average='micro')
recall = recall_score(true, predicted, average='micro')
f1 = f1_score(true, predicted, average='micro')
conf_matrix = confusion_matrix(true, predicted)

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:")
print(conf_matrix)



class ELMo_last_layer(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, weights_matrix):
        super(ELMo_last_layer, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.embedding_layer = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)
        self.scalars = nn.ParameterList([nn.Parameter(torch.ones(1)) for _ in range(num_layers+1)]).double()
        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.lstm2 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.scalars = nn.ParameterList([nn.Parameter(torch.ones(1)) for _ in range(1)]).double()
        self.output_layer = nn.Linear(embedding_dim, vocab_size).double()
        # self.alphas = [0.2, 0.4, 0.4]

    def forward(self, x):
        embedded_x = self.embedding_layer(x)
        lstm_out1, _ = self.lstm1(embedded_x)
        lstm_out2, _ = self.lstm2(lstm_out1)
        # print("lstm2 : ", lstm_out2.shape)
        final_out = self.output_layer(lstm_out2)

        embed_out = lstm_out2*self.scalars[0]
        return final_out, embed_out

model_ELMo_last_layer = ELMo_last_layer(vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix).to(device)

loss_function = nn.CrossEntropyLoss()
learning_rate = 0.001
optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)

num_epochs = 2
path = "/content/gdrive/My Drive/ass2_model_last_parameters.pth"
prev_loss = 1000
from tqdm import tqdm

for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count=0
    mini = 100
    for batch_input_embeddings, batch_target_embeddings in (dataloader):
        loss = 0
        batch_input_embeddings = batch_input_embeddings.to(device)
        batch_target_embeddings = batch_target_embeddings.to(device)

        # print(batch_input_embeddings.shape, batch_target_embeddings.shape)
        optimizer.zero_grad()
        outputs, _ = model_ELMo_last_layer(torch.tensor(batch_input_embeddings))
        # outputs = outputs.float()
        # print(batch_target_embeddings.shape, outputs.shape)
        loss = loss_function(outputs.view(-1,vocab_size),batch_target_embeddings.contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    if(prev_loss > total_loss):
        torch.save(model_ELMo_last_layer.state_dict(), path)
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader)}')



model_downstream_last_layer = downstream_task(embedding_dim, vocab_size, 4).to(device)
no_of_values = 4
loss_function = nn.CrossEntropyLoss()
learning_rate = 0.001
optimizer = optim.Adam(model_downstream_last_layer.parameters(), lr=learning_rate, weight_decay=1e-5)



count = 0
total = 0

true = []
predicted = []
# for batch_input_embeddings, batch_target in tqdm(dataloader_downstream_dev):

for batch_input_embeddings, batch_target in dataloader_downstream_dev:
    batch_input_embeddings = batch_input_embeddings.to(device)
    batch_target = batch_target.to(device)
    _, outputs = model_ELMo_last_layer(batch_input_embeddings)
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]

    outputs_downstream = model_downstream_last_layer(outputs_sum).cpu().detach().numpy()

    # print("Input 1 : ", batch_input_embeddings.shape)
    # print("Output 1 : ", outputs.shape)
    # print(batch_input_embeddings)
    outputs_down = np.argmax((torch.tensor(outputs_downstream)).cpu().numpy())
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
    # outputs_sum = outputs_sum.cpu().numpy()
    # print("argmax : ", outputs_down)

    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()
    # print("Output 2 : ", outputs_downstream.shape)



    true.append(batch_target.cpu())  # Move the true labels to CPU
    predicted.append(outputs_down)
    if outputs_down == batch_target:
        count += 1
    total += 1
    if(total == 10000):
        break
    if(total%100 == 0):
        print(count / total)

print("accuracy : ", count / total)

true = [tensor.item() for tensor in true]

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# Calculate precision, recall, and F1 score with 'micro' average
precision = precision_score(true, predicted, average='micro')
recall = recall_score(true, predicted, average='micro')
f1 = f1_score(true, predicted, average='micro')
conf_matrix = confusion_matrix(true, predicted)

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:")
print(conf_matrix)

"""BONUS TASKS

ALMOST EQUAL WEIGHTS TO ALL EMBEDDINGS  [0.3, 0.3, 0.4]
"""

class ELMo_1(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, weights_matrix):
        super(ELMo_1, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.embedding_layer = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)
        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.lstm2 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.output_layer = nn.Linear(embedding_dim, vocab_size).double()
        self.alphas = [0.3, 0.3, 0.4]

    def forward(self, x):
        embedded_x = self.embedding_layer(x)
        lstm_out1, _ = self.lstm1(embedded_x)
        # print("lstm1 : ", lstm_out1.shape)
        lstm_out2, _ = self.lstm2(lstm_out1)
        # print("lstm2 : ", lstm_out2.shape)
        final_out = self.output_layer(lstm_out2)

        embed_out = embedded_x*self.alphas[0] + lstm_out1*self.alphas[1] + lstm_out2*self.alphas[2]
        return final_out, embed_out

import torch
import torch.optim as optim
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence
embedding_dim = 100
hidden_dim = 50
num_layers = 2
batch_size_lstm = 32
vocab_size = len(vocabulary)
embedding_matrix = embedding_matrix.to(device)
model_1 = ELMo_1(vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix).to(device)

# Define loss function and optimizer
loss_function = nn.CrossEntropyLoss()
learning_rate = 0.001
optimizer = optim.Adam(model_1.parameters(), lr=0.01, weight_decay=1e-5)

num_epochs = 2
path = "/content/gdrive/My Drive/ass2_model_1.pth"
prev_loss = 1000
from tqdm import tqdm

for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count=0
    mini = 100
    for batch_input_embeddings, batch_target_embeddings in (dataloader):
        loss = 0
        batch_input_embeddings = batch_input_embeddings.to(device)
        batch_target_embeddings = batch_target_embeddings.to(device)

        # print(batch_input_embeddings.shape, batch_target_embeddings.shape)
        optimizer.zero_grad()
        outputs, _ = model_1(torch.tensor(batch_input_embeddings))
        outputs = outputs.float()
        # print(batch_target_embeddings.shape, outputs.shape)
        loss = loss_function(outputs.view(-1,vocab_size),batch_target_embeddings.contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    if(prev_loss > total_loss):
        torch.save(model_1.state_dict(), path)
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader)}')

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class downstream_task(nn.Module):
    def __init__(self, embedding_dim, vocab_size, no_of_values, num_layers=2):
        super(downstream_task, self).__init__()
        self.vocab_size = vocab_size
        self.no_of_values = no_of_values
        # self.lstm = nn.LSTM(embedding_dim, embedding_dim, num_layers, batch_first=True).double()
        self.linear = nn.Linear(embedding_dim, no_of_values).double()

    def forward(self, input):
        # lstm_out, (hidden, cell_state) = self.lstm(input)
        # print("hidden shape", hidden.shape)
        # linear_out = self.linear(hidden[1])
        # print("linear_out shape", linear_out.shape)

        linear_out = self.linear(input)
        return linear_out

model_downstream = downstream_task(embedding_dim, vocab_size, 4).to(device)

# num_params = sum(p.numel() for p in model_downstream.parameters() if p.requires_grad)
# print(f"Number of trainable parameters: {num_params}")

no_of_values = 4

loss_function = nn.CrossEntropyLoss()
learning_rate = 0.001
optimizer = optim.Adam(model_downstream.parameters(), lr=learning_rate, weight_decay=1e-5)
# train_val = [int(val)-1 for val in train_value]

import torch

def custom_collate_fn(batch):
    max_length = max(len(sentence) for sentence, _ in batch)
    padded_inputs = []
    outputs = []
    for sentence, label in batch:
        # print(sentence)
        sos_token = [word2idx['<SOS>']]
        sentence_tokens = [word2idx[word] if word in word2idx else word2idx['<UNK>'] for word in sentence]
        pad_tokens = [word2idx['<PAD>']] * (max_length - len(sentence))
        combined_list = sos_token + sentence_tokens + pad_tokens

        padded_inputs.append(combined_list)
        outputs.append(label)


    # print(torch.tensor(padded_inputs).shape, torch.tensor(outputs).shape)
    return torch.tensor(padded_inputs), torch.tensor(outputs)


from torch.utils.data import DataLoader, Dataset
batch_size = 32

class CustomDataset(Dataset):
    def __init__(self, sentences, labels):
        self.sentences = sentences
        self.labels = labels

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        return [self.sentences[idx], self.labels[idx]]

batch_size = 1
custom_dataset = CustomDataset(train_sentence, train_val)
dataloader_downstream = DataLoader(custom_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)

for batch_input_embeddings, batch_target in (dataloader_downstream):
    print(batch_input_embeddings, batch_target)
    print(batch_input_embeddings.shape, batch_target.shape)
    break

num_epochs = 2
prev_loss = 1000
from tqdm import tqdm
for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count = 0
    mini = 100
    for batch_input_embeddings, batch_target in (dataloader_downstream):
        loss = 0
        batch_input_embeddings = torch.tensor(batch_input_embeddings).to(device)
        batch_target = batch_target.to(device)
        optimizer.zero_grad()
        _, outputs = model_1(batch_input_embeddings)
        # print("outputs",outputs.shape)

        outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
        # print("outputs_downstream",outputs_downstream.shape)
        outputs_downstream = model_downstream(outputs_sum)
        # print("outputs_downstream",outputs_downstream.shape)
        loss = loss_function(outputs_downstream, batch_target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if prev_loss > total_loss:
        torch.save(model_downstream.state_dict(), "/content/gdrive/My Drive/ass2_downstream_model_1.pth")
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader_downstream)}')

count = 0
total = 0

true = []
predicted = []
# for batch_input_embeddings, batch_target in tqdm(dataloader_downstream_dev):

for batch_input_embeddings, batch_target in dataloader_downstream:
    batch_input_embeddings = batch_input_embeddings.to(device)
    batch_target = batch_target.to(device)
    _, outputs = model_1(batch_input_embeddings)
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]

    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()

    # print("Input 1 : ", batch_input_embeddings.shape)
    # print("Output 1 : ", outputs.shape)
    # print(batch_input_embeddings)
    outputs_down = np.argmax((torch.tensor(outputs_downstream)).cpu().numpy())
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()

    true.append(batch_target.cpu())
    predicted.append(outputs_down)
    if outputs_down == batch_target:
        count += 1
    total += 1
    if(total == 10000):
        break
    if(total%100 == 0):
        print(count / total)

print("accuracy : ", count / total)

true = [x.item() for x in true]
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

# Calculate precision, recall, and F1 score with 'micro' average
precision = precision_score(true, predicted, average='micro')
recall = recall_score(true, predicted, average='micro')
f1 = f1_score(true, predicted, average='micro')
conf_matrix = confusion_matrix(true, predicted)

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:")
print(conf_matrix)

"""MORE WEIGHTAGE TO LAST LAYER"""

class ELMo_2(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, weights_matrix):
        super(ELMo_2, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.embedding_layer = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)
        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.lstm2 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.output_layer = nn.Linear(embedding_dim, vocab_size).double()
        self.alphas = [0.1, 0.3, 0.6]

    def forward(self, x):
        embedded_x = self.embedding_layer(x)
        lstm_out1, _ = self.lstm1(embedded_x)
        # print("lstm1 : ", lstm_out1.shape)
        lstm_out2, _ = self.lstm2(lstm_out1)
        # print("lstm2 : ", lstm_out2.shape)
        final_out = self.output_layer(lstm_out2)

        embed_out = embedded_x*self.alphas[0] + lstm_out1*self.alphas[1] + lstm_out2*self.alphas[2]
        return final_out, embed_out

model_2 = ELMo_2(vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix).to(device)

# Define loss function and optimizer
loss_function = nn.CrossEntropyLoss()
learning_rate = 0.001
optimizer = optim.Adam(model_2.parameters(), lr=0.01, weight_decay=1e-5)

num_epochs = 2
path = "/content/gdrive/My Drive/ass2_model_2.pth"
prev_loss = 1000
from tqdm import tqdm

for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count=0
    mini = 100
    for batch_input_embeddings, batch_target_embeddings in (dataloader):
        loss = 0
        batch_input_embeddings = batch_input_embeddings.to(device)
        batch_target_embeddings = batch_target_embeddings.to(device)

        # print(batch_input_embeddings.shape, batch_target_embeddings.shape)
        optimizer.zero_grad()
        outputs, _ = model_2(torch.tensor(batch_input_embeddings))
        outputs = outputs.float()
        # print(batch_target_embeddings.shape, outputs.shape)
        loss = loss_function(outputs.view(-1,vocab_size),batch_target_embeddings.contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    if(prev_loss > total_loss):
        torch.save(model_2.state_dict(), path)
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader)}')

num_epochs = 2
prev_loss = 1000
from tqdm import tqdm
for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count = 0
    mini = 100
    for batch_input_embeddings, batch_target in (dataloader_downstream):
        loss = 0
        batch_input_embeddings = torch.tensor(batch_input_embeddings).to(device)
        batch_target = batch_target.to(device)
        optimizer.zero_grad()
        _, outputs = model_2(batch_input_embeddings)
        # print("outputs",outputs.shape)

        outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
        # print("outputs_downstream",outputs_downstream.shape)
        outputs_downstream = model_downstream(outputs_sum)
        # print("outputs_downstream",outputs_downstream.shape)
        loss = loss_function(outputs_downstream, batch_target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if prev_loss > total_loss:
        torch.save(model_downstream.state_dict(), "/content/gdrive/My Drive/ass2_downstream_model_2.pth")
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader_downstream)}')

count = 0
total = 0

true = []
predicted = []
# for batch_input_embeddings, batch_target in tqdm(dataloader_downstream_dev):

for batch_input_embeddings, batch_target in dataloader_downstream:
    batch_input_embeddings = batch_input_embeddings.to(device)
    batch_target = batch_target.to(device)
    _, outputs = model_2(batch_input_embeddings)
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]

    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()
    outputs_down = np.argmax((torch.tensor(outputs_downstream)).cpu().numpy())
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()

    true.append(batch_target.cpu())
    predicted.append(outputs_down)
    if outputs_down == batch_target:
        count += 1
    total += 1
    if(total == 1000):
        break
    if(total%100 == 0):
        print(count / total)

print("accuracy : ", count / total)

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
true = [x.item() for x in true]
# Calculate precision, recall, and F1 score with 'micro' average
precision = precision_score(true, predicted, average='micro')
recall = recall_score(true, predicted, average='micro')
f1 = f1_score(true, predicted, average='micro')
conf_matrix = confusion_matrix(true, predicted)

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:")
print(conf_matrix)

"""MODEL WITH MORE WEIGHTAGE TO EMBEDDING INPUTS




"""

class ELMo_3(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, weights_matrix):
        super(ELMo_3, self).__init__()
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.embedding_layer = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)
        self.lstm1 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.lstm2 = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, bidirectional=True, batch_first=True).double()
        self.output_layer = nn.Linear(embedding_dim, vocab_size).double()
        self.alphas = [0.7, 0.2, 0.1]

    def forward(self, x):
        embedded_x = self.embedding_layer(x)
        lstm_out1, _ = self.lstm1(embedded_x)
        # print("lstm1 : ", lstm_out1.shape)
        lstm_out2, _ = self.lstm2(lstm_out1)
        # print("lstm2 : ", lstm_out2.shape)
        final_out = self.output_layer(lstm_out2)

        embed_out = embedded_x*self.alphas[0] + lstm_out1*self.alphas[1] + lstm_out2*self.alphas[2]
        return final_out, embed_out

model_3 = ELMo_3(vocab_size, embedding_dim, hidden_dim, num_layers, embedding_matrix).to(device)

# Define loss function and optimizer
loss_function = nn.CrossEntropyLoss()
learning_rate = 0.001
optimizer = optim.Adam(model_3.parameters(), lr=0.01, weight_decay=1e-5)

num_epochs = 2
path = "/content/gdrive/My Drive/ass2_model_3.pth"
prev_loss = 1000
from tqdm import tqdm

for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count=0
    mini = 100
    for batch_input_embeddings, batch_target_embeddings in (dataloader):
        loss = 0
        batch_input_embeddings = batch_input_embeddings.to(device)
        batch_target_embeddings = batch_target_embeddings.to(device)

        # print(batch_input_embeddings.shape, batch_target_embeddings.shape)
        optimizer.zero_grad()
        outputs, _ = model_3(torch.tensor(batch_input_embeddings))
        outputs = outputs.float()
        # print(batch_target_embeddings.shape, outputs.shape)
        loss = loss_function(outputs.view(-1,vocab_size),batch_target_embeddings.contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    if(prev_loss > total_loss):
        torch.save(model_3.state_dict(), path)
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader)}')

num_epochs = 2
prev_loss = 1000
from tqdm import tqdm
for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count = 0
    mini = 100
    for batch_input_embeddings, batch_target in (dataloader_downstream):
        loss = 0
        batch_input_embeddings = torch.tensor(batch_input_embeddings).to(device)
        batch_target = batch_target.to(device)
        optimizer.zero_grad()
        _, outputs = model_3(batch_input_embeddings)
        # print("outputs",outputs.shape)

        outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
        # print("outputs_downstream",outputs_downstream.shape)
        outputs_downstream = model_downstream(outputs_sum)
        # print("outputs_downstream",outputs_downstream.shape)
        loss = loss_function(outputs_downstream, batch_target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    if prev_loss > total_loss:
        torch.save(model_downstream.state_dict(), "/content/gdrive/My Drive/ass2_downstream_model_3.pth")
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader_downstream)}')

count = 0
total = 0

true = []
predicted = []
# for batch_input_embeddings, batch_target in tqdm(dataloader_downstream_dev):


for batch_input_embeddings, batch_target in dataloader_downstream:
    batch_input_embeddings = batch_input_embeddings.to(device)
    batch_target = batch_target.to(device)
    _, outputs = model_2(batch_input_embeddings)
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]

    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()

    # print("Input 1 : ", batch_input_embeddings.shape)
    # print("Output 1 : ", outputs.shape)
    # print(batch_input_embeddings)
    outputs_down = np.argmax((torch.tensor(outputs_downstream)).cpu().numpy())
    outputs_sum = torch.sum(outputs, dim=1)/outputs.shape[1]
    # outputs_sum = outputs_sum.cpu().numpy()
    # print("argmax : ", outputs_down)

    outputs_downstream = model_downstream(outputs_sum).cpu().detach().numpy()
    # print("Output 2 : ", outputs_downstream.shape)



    true.append(batch_target.cpu())  # Move the true labels to CPU
    predicted.append(outputs_down)
    if outputs_down == batch_target:
        count += 1
    total += 1
    if(total == 1000):
        break
    if(total%100 == 0):
        print(count / total)

print("accuracy : ", count / total)

from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix
true = [x.item() for x in true]
# Calculate precision, recall, and F1 score with 'micro' average
precision = precision_score(true, predicted, average='micro')
recall = recall_score(true, predicted, average='micro')
f1 = f1_score(true, predicted, average='micro')
conf_matrix = confusion_matrix(true, predicted)

print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("Confusion Matrix:")
print(conf_matrix)

"""character convolutional network in ELM0"""

char_train = train_sentence
char_label = train_value

char_vocabulary = set()
char2idx = {}
idx2char = {}

for sentence in char_train:
    for word in sentence:
        for char in word:
            char_vocabulary.add(char)

char_to_index = {char: idx for idx, char in enumerate(char_vocabulary)}
char_vocab_size = len(char_vocabulary)
def get_one_hot_encode(char, char_vocabulary_size):
    one_hot = torch.zeros(char_vocabulary_size)
    one_hot[char_to_index.get(char, 0)] = 1
    return one_hot

char_vector = get_one_hot_encode('b', len(char_vocabulary))
print(char_vector)

char_embedding_dim = 10
char_hidden_dim = 20

char_embedding_layer = nn.Embedding(len(char_vocabulary), char_embedding_dim)

char_embedding = char_embedding_layer(torch.LongTensor([char_to_index['b']]))
print(char_embedding)

import torch
import torch.nn as nn
import torch.nn.functional as F

class CharCNN(nn.Module):
    def __init__(self, vocab_size, char_embedding_dim, char_hidden_dim):
        super(CharCNN, self).__init__()
        self.char_embedding = nn.Embedding(vocab_size, char_embedding_dim)
        self.char_conv1 = nn.Conv1d(char_embedding_dim, char_hidden_dim, kernel_size=3)
        self.char_conv2 = nn.Conv1d(char_embedding_dim, char_hidden_dim, kernel_size=4)
        self.char_conv3 = nn.Conv1d(char_embedding_dim, char_hidden_dim, kernel_size=5)

    def forward(self, x):
        char_embedded = self.char_embedding(x)
        char_embedded = char_embedded.permute(0, 2, 1)  # Swap dimensions for Conv1d
        char_out1 = F.relu(self.char_conv1(char_embedded))
        char_out2 = F.relu(self.char_conv2(char_embedded))
        char_out3 = F.relu(self.char_conv3(char_embedded))
        char_out1 = F.max_pool1d(char_out1, char_out1.size(2)).squeeze(2)
        char_out2 = F.max_pool1d(char_out2, char_out2.size(2)).squeeze(2)
        char_out3 = F.max_pool1d(char_out3, char_out3.size(2)).squeeze(2)
        char_out = torch.cat([char_out1, char_out2, char_out3], dim=1)
        return char_out

class ELMoChar(nn.Module):
    def __init__(self, vocab_size, char_vocab_size, char_embedding_dim, char_hidden_dim, hidden_dim):
        super(ELMoChar, self).__init__()
        self.embedding_dim = char_hidden_dim * 3  # Concatenate character CNN output
        self.hidden_dim = hidden_dim
        self.char_cnn = CharCNN(char_vocab_size, char_embedding_dim, char_hidden_dim)
        self.lstm1 = nn.LSTM(input_size=self.embedding_dim, hidden_size=hidden_dim, dropout=0.3, bidirectional=True)
        self.lstm2 = nn.LSTM(input_size=self.embedding_dim, hidden_size=hidden_dim, dropout=0.3, bidirectional=True)
        self.output_layer = nn.Linear(self.embedding_dim, vocab_size)

    def forward(self, char_inputs):
        char_cnn_out = self.char_cnn(char_inputs)
        lstm_out1, _ = self.lstm1(char_cnn_out.unsqueeze(1))  # Add an additional dimension
        lstm_out2, _ = self.lstm2(lstm_out1)
        final_out = self.output_layer(lstm_out2)
        return final_out

model_ELMoChar = ELMoChar(vocab_size, char_vocab_size, char_embedding_dim, char_hidden_dim, hidden_dim).to(device)

# Define loss function and optimizer
loss_function = nn.CrossEntropyLoss()
# learning_rate = 0.001
optimizer = optim.Adam(model_ELMoChar.parameters(), lr=0.01, weight_decay=1e-5)

num_epochs = 5
path = "/content/gdrive/My Drive/ass2_model_dropout_char.pth"
prev_loss = 1000
from tqdm import tqdm

for epoch in tqdm(range(num_epochs)):
    total_loss = 0
    count=0
    mini = 100
    for batch_input_embeddings, batch_target_embeddings in (dataloader):
        loss = 0
        batch_input_embeddings = batch_input_embeddings.to(device)
        batch_target_embeddings = batch_target_embeddings.to(device)

        # print(batch_input_embeddings.shape, batch_target_embeddings.shape)
        optimizer.zero_grad()
        outputs, _ = model_ELMoChar(torch.tensor(batch_input_embeddings))
        # print(batch_target_embeddings.shape, outputs.shape)
        loss = loss_function(outputs.view(-1,vocab_size),batch_target_embeddings.contiguous().view(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    if(prev_loss > total_loss):
        torch.save(model.state_dict(), path)
    prev_loss = total_loss
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader)}')



